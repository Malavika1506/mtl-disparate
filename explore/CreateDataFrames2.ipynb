{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import functions as fun\n",
    "from collections import Counter\n",
    "import ast\n",
    "import pickle\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import pydot\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import odeint \n",
    "import pandas as pd\n",
    "import scipy as sp \n",
    "import string\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "import sklearn.model_selection as ms\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import tensorboard as ts\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "#tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "h = ['claimID', 'claim', 'label', 'claimURL', 'reason', 'categories', 'speaker', 'checker', 'tags', 'articleTitle', 'publishDate', 'claimDate', 'entities']\n",
    "df_train = pd.read_csv('../data/train.tsv',sep='\\t',names=h,header=None)\n",
    "df_dev = pd.read_csv('../data/dev.tsv',sep='\\t',names=h,header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>snip_1</th>\n",
       "      <th>snip_2</th>\n",
       "      <th>snip_url</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claimID</th>\n",
       "      <th>snip_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">abbc-00001</th>\n",
       "      <th>2</th>\n",
       "      <td>Record numbers march down Brisbane city to pro...</td>\n",
       "      <td>Sep 4, 2018 ... THOUSANDS of people joined the...</td>\n",
       "      <td>http://catholicleader.com.au/news/record-numbe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fact check: Will Queensland's proposed abortio...</td>\n",
       "      <td>Oct 3, 2018 ... Protestors have marched throug...</td>\n",
       "      <td>http://abortion-news.info/fact-check-will-quee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Queensland Council for Civil Liberties - Wikip...</td>\n",
       "      <td>The Queensland Council for Civil Liberties (QC...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Queensland_Counc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Abortion law change expected from Queensland L...</td>\n",
       "      <td>Jun 28, 2018 ... The Queensland Law Reform Com...</td>\n",
       "      <td>http://www.abc.net.au/news/2018-06-29/abortion...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Struggle</td>\n",
       "      <td>enforcing the State‟s controversial anti-march...</td>\n",
       "      <td>https://www.childrenbychoice.org.au/images/dow...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               snip_1  \\\n",
       "claimID    snip_id                                                      \n",
       "abbc-00001 2        Record numbers march down Brisbane city to pro...   \n",
       "           3        Fact check: Will Queensland's proposed abortio...   \n",
       "           4        Queensland Council for Civil Liberties - Wikip...   \n",
       "           5        Abortion law change expected from Queensland L...   \n",
       "           6                                             The Struggle   \n",
       "\n",
       "                                                               snip_2  \\\n",
       "claimID    snip_id                                                      \n",
       "abbc-00001 2        Sep 4, 2018 ... THOUSANDS of people joined the...   \n",
       "           3        Oct 3, 2018 ... Protestors have marched throug...   \n",
       "           4        The Queensland Council for Civil Liberties (QC...   \n",
       "           5        Jun 28, 2018 ... The Queensland Law Reform Com...   \n",
       "           6        enforcing the State‟s controversial anti-march...   \n",
       "\n",
       "                                                             snip_url  \n",
       "claimID    snip_id                                                     \n",
       "abbc-00001 2        http://catholicleader.com.au/news/record-numbe...  \n",
       "           3        http://abortion-news.info/fact-check-will-quee...  \n",
       "           4        https://en.wikipedia.org/wiki/Queensland_Counc...  \n",
       "           5        http://www.abc.net.au/news/2018-06-29/abortion...  \n",
       "           6        https://www.childrenbychoice.org.au/images/dow...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = ['snip_id', 'snip_1', 'snip_2','snip_url']\n",
    "df_snip =  pd.DataFrame(columns=h)\n",
    "\n",
    "\n",
    "dir = '..\\data\\snippets'\n",
    "for filename in os.listdir(dir):\n",
    "    df_row = pd.read_csv(os.path.join(dir, filename),names=h,sep='\\t',header=None,engine='python',encoding='utf8' ,quoting=3)\n",
    "    df_row.insert(loc = 0, column = 'claimID', value = filename )\n",
    "    \n",
    "    df_snip = df_snip.append(df_row)\n",
    "\n",
    "df_snip.set_index(['claimID','snip_id'],inplace = False).head()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove rows with missing snippets + clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_snip.reset_index(inplace=True)\n",
    "df_train.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (24004, 13)\n",
      "dev shape: (2988, 12)\n",
      "Snip shape: (274657, 6) Unique: 30100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_train = df_train.loc[df_train['claimID'].isin(df_snip['claimID'].unique())]\n",
    "df_dev =df_dev.loc[df_dev['claimID'].isin(df_snip['claimID'].unique())]\n",
    "\n",
    "\n",
    "df_train.replace(to_replace = 'None',value=np.nan ,inplace = True)\n",
    "df_train.replace(to_replace = \"['None']\",value=np.nan ,inplace = True)\n",
    "\n",
    "df_dev.replace(to_replace = 'None',value=np.nan ,inplace = True)\n",
    "df_dev.replace(to_replace = \"['None']\",value=np.nan ,inplace = True)\n",
    "\n",
    "df_snip['snip_url'] = df_snip.apply(lambda x: fun.clean_url(x.snip_url),axis=1)\n",
    "\n",
    "print('train shape:',np.shape(df_train))\n",
    "print('dev shape:',np.shape(df_dev))\n",
    "print('Snip shape:',np.shape(df_snip),'Unique:',len(df_snip['claimID'].unique()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data to pickle  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train.set_index('claimID',inplace = True)\n",
    "df_snip.set_index(['claimID','snip_id'],inplace = True)\n",
    "df_dev.set_index('claimID',inplace = True)\n",
    "\n",
    "df_snip.to_pickle(\"./df_snip.pkl\")\n",
    "df_train.to_pickle(\"./df_dev.pkl\")\n",
    "df_train.to_pickle(\"./df_train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_snip = pd.read_pickle(\"./df_snip.pkl\")\n",
    "#df_train = pd.read_pickle(\"./df_train.pkl\")\n",
    "\n",
    "#df_train.reset_index(drop=False, inplace=True)\n",
    "#df_snip.reset_index(drop=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_num =  df_train['label'].astype(\"category\").cat.codes.values\n",
    "\n",
    "with open('y_train_num','wb') as i:\n",
    "       pickle.dump(y_train_num, i) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = df_train[['claimURL','speaker','checker']].copy() \n",
    "\n",
    "\n",
    "df_meta['claimURL'] = df_meta.apply(lambda x: fun.clean_url(x.claimURL),axis=1)\n",
    "\n",
    "df_meta['claimURL'] = df_meta['claimURL'].astype(\"category\").cat.codes.values\n",
    "\n",
    "df_meta['speaker'] = df_meta['speaker'].astype(\"category\").cat.codes.values\n",
    "df_meta['checker'] = df_meta['checker'].astype(\"category\").cat.codes.values\n",
    "\n",
    "    \n",
    "df_meta.head() \n",
    "\n",
    "X_metadata = df_meta.to_numpy()\n",
    "\n",
    "bin_enteties = fun.create_binary(df_train['entities'],30)\n",
    "\n",
    "tags = df_train.apply(lambda x: str(x.tags).replace('[','').replace(']','').replace(\"'\",'').split(',')   ,axis=1)\n",
    "\n",
    "bin_tags = fun.create_binary(tags,30)\n",
    "\n",
    "\n",
    "\n",
    "X_metadata = np.append(X_metadata, bin_enteties, axis=1)\n",
    "\n",
    "with open('X_meta','wb') as i:\n",
    "       pickle.dump(X_metadata, i)\n",
    "\n",
    "\n",
    "df_meta_num = pd.DataFrame(data = np.hstack((df_train.index.to_numpy().reshape(-1,1),X_metadata))     )        \n",
    "        \n",
    "with open('df_meta_num','wb') as i:\n",
    "       pickle.dump(df_meta_num, i)        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claim set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_claim = df_train['claim'].str.lower().copy()\n",
    "\n",
    "words_list = [i if type(i)==str else '' for i in df_claim]\n",
    "\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(words_list)\n",
    "\n",
    "sequences = t.texts_to_sequences(words_list)\n",
    "\n",
    "X_claim = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=50)\n",
    "\n",
    "\n",
    "with open('X_claim','wb') as i:\n",
    "       pickle.dump(X_claim, i)\n",
    "            \n",
    "\n",
    "            \n",
    "df_claim_num = pd.DataFrame(data = np.hstack((df_train.index.to_numpy().reshape(-1,1),X_claim))   )        \n",
    "        \n",
    "with open('df_claim_num','wb') as i:\n",
    "       pickle.dump(df_claim_num, i) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snippet set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_snip['snip_url'] = df_snip.apply(lambda x: fun.clean_url(x.snip_url),axis=1)\n",
    "\n",
    "df_snip['snip_1'] = df_snip['snip_1'].str.lower()\n",
    "df_snip['snip_2'] = df_snip['snip_2'].str.lower()\n",
    "\n",
    "df_snip['snip_url'] = df_snip['snip_url'].astype(\"category\").cat.codes.values\n",
    "\n",
    "\n",
    "df_snip.reset_index(drop=False, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "words_list1 = [i if type(i)==str else '' for i in df_snip['snip_1']]\n",
    "sequences1 = t.texts_to_sequences(words_list1)\n",
    "snip1 = tf.keras.preprocessing.sequence.pad_sequences(sequences1, maxlen=20)\n",
    "\n",
    "words_list2 = [i if type(i)==str else '' for i in df_snip['snip_2']]\n",
    "sequences2 = t.texts_to_sequences(words_list2)\n",
    "snip2 = tf.keras.preprocessing.sequence.pad_sequences(sequences2, maxlen=80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "snip_info = df_snip[['claimID','snip_id','snip_url']].to_numpy()\n",
    "snip = np.append(snip_info, np.append(snip1,snip2,axis=1), axis=1)\n",
    "\n",
    "\n",
    "df_snip_num =pd.DataFrame(data=snip)\n",
    "df_snip_num\n",
    "\n",
    "with open('df_snip_num','wb') as i:\n",
    "       pickle.dump(df_snip_num, i) \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_snip_num.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flat Snippet set (very very slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "snippet_list = []\n",
    "\n",
    "for q,i in enumerate(df_train.index):\n",
    "    pass\n",
    "    row = []\n",
    "    for i,r in df_snip_num[ df_snip_num[0]==i].iterrows():\n",
    "       \n",
    "        row.append(r[1:].to_list())\n",
    "    row = [snip for snippets in row for snip in snippets]\n",
    "    \n",
    "    snippet_list.append(row)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_snip_flat =  tf.keras.preprocessing.sequence.pad_sequences(snippet_list, maxlen=1005)\n",
    "\n",
    "with open('X_snip_flat','wb') as i:\n",
    "       pickle.dump(X_snip_flat, i) \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_claim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
